{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end demonstration of RAG development\n",
    "- Includes development of Vector store that is saved as storage\n",
    "- Develops two query engine, used to process queries based on FT+RAG and Baseline + RAG\n",
    "\n",
    "It is recommended to skip the 'building vector store with documents' section and move directly to load vector store.\n",
    "\n",
    "If it is wished build the vector store it is neccessary to retrieve the following files and move to datasets\n",
    "    \n",
    "    \"datasets/headphones_1of2.json\",\n",
    "    \n",
    "    \"datasets/headphones_2of2.json\",\n",
    "    \n",
    "    \"datasets/speakers_1of2.json\",\n",
    "    \n",
    "    \"datasets/all_models_appended.csv\",\n",
    "    \n",
    "    \"datasets/Question-for-second-evaluaton.xlsx\"\n",
    "\n",
    "#### For testing individual queries allows to run single queries based on the query engine\n",
    "#### Evaluation of dataset section automatically develops an excel file with question evaluations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from llama_index.core import Document\n",
    "import faiss\n",
    "import openai\n",
    "import pandas as pd\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import json\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    load_index_from_storage,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from IPython.display import Markdown, display\n",
    "API_KEY = 'insert api key here'\n",
    "openai.api_key = API_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building vector store with documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/zjh9b6rs2hb_pv6wtvvf6gbw0000gn/T/ipykernel_65625/3022546896.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dff['full_qa'] = dff['llm_question'] + ' ' + dff['llm_answer']\n"
     ]
    }
   ],
   "source": [
    "#### FULL DATA LOADERS\n",
    "\n",
    "# dimensions of text-ada-embedding-002\n",
    "d = 1536\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "\n",
    "#Writing 'sections' dictionary to a JSON file\n",
    "with open(\"datasets/headphones_1of2.json\", 'r', encoding='utf-8') as f:\n",
    "    faqs_1 = json.load(f)\n",
    "with open(\"datasets/headphones_2of2.json\", 'r', encoding='utf-8') as f:\n",
    "    faqs_2 = json.load(f)\n",
    "with open(\"datasets/speakers_1of2.json\", 'r', encoding='utf-8') as f:\n",
    "    faqs_3 = json.load(f)\n",
    "\n",
    "\n",
    "df = pd.read_csv('datasets/all_models_appended.csv', delimiter='~')\n",
    "\n",
    "dff = df[df['model'] == 'gpt3.5e']\n",
    "dff['full_qa'] = dff['llm_question'] + ' ' + dff['llm_answer']\n",
    "\n",
    "faqs_beoworld = dff['full_qa'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1796\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "i += len(faqs_1)\n",
    "i += len(faqs_2)\n",
    "i += len(faqs_3)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "# Handling FAQs\n",
    "for faqs in [faqs_1,faqs_2,faqs_3]:\n",
    "    for q,v in faqs.items():\n",
    "        doc = Document(text=q+' '+v)\n",
    "        documents.append(doc)\n",
    "\n",
    "for d in faqs_beoworld:\n",
    "    doc = Document(text=d)\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index to disk\n",
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load index from disk\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./storage\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./storage/\"\n",
    ")\n",
    "index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3393"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index.index_struct.nodes_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For testing individual queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press and hold the Bluetooth button for two seconds on your BeoSound A5 to initiate the Bluetooth connection. Once the light indicator starts flashing blue, your BeoSound A5 is ready to pair with your device. Remember to open the Bluetooth settings on your device to complete the pairing process.\n",
      "['Beosound\\u202fA5\\u202fBluetooth\\u202fconnection  Switch on\\u202fBeosound\\u202fA5. Press and hold\\u202fthe Bluetooth\\u202fbutton for\\u202f\\u202ftwo\\u202fseconds.\\u202fThe\\u202flight indicator will start flashing blue, and your\\u202fBeosound\\u202fA5 is ready to connect to your device (phone, computer, tablet, etc.). \\n--- \\n---Open the Bluetooth\\u202fsettings on your device and pair with the speaker.', 'Beoplay P2 Bluetooth connection']\n"
     ]
    }
   ],
   "source": [
    "# query engine vanilla gpt35\n",
    "llm_gpt35 = OpenAI(model='gpt-3.5-turbo-0125', api_key=API_KEY, temperature=1)\n",
    "query_engine_gpt35 = index.as_query_engine(similarity_top_k=2, llm=llm_gpt35)\n",
    "\n",
    "# query engine finetuned gpt35\n",
    "llm_ft = OpenAI(model=\"ft:gpt-3.5-turbo-0125:personal:master:9Bfq5w4r\", api_key=API_KEY, temperature=1)\n",
    "query_engine_ft = index.as_query_engine(similarity_top_k=2, llm=llm_ft)\n",
    "\n",
    "def rag_query(query_engine, query):\n",
    "    response = query_engine.query(query)\n",
    "    context = [node.text for node in response.source_nodes]\n",
    "    return response.response, context\n",
    "\n",
    "query = \"I cant connect to bluetooh on my BeoSound A5 \"\n",
    "response, context = rag_query(query_engine_gpt35, query)\n",
    "print(response)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beosound\\u202fA5\\u202fBluetooth\\u202fconnection  Switch on\\u202fBeosound\\u202fA5. Press and hold\\u202fthe Bluetooth\\u202fbutton for\\u202f\\u202ftwo\\u202fseconds.\\u202fThe\\u202flight indicator will start flashing blue, and your\\u202fBeosound\\u202fA5 is ready to connect to your device (phone, computer, tablet, etc.). \\n--- \\n---Open the Bluetooth\\u202fsettings on your device and pair with the speaker.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_synthesizer:text_qa_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: '), conditionals=[(<function is_chat_model at 0x130472840>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content=\"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\", additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: ', additional_kwargs={})]))]),\n",
       " 'response_synthesizer:refine_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template=\"The original query is as follows: {query_str}\\nWe have provided an existing answer: {existing_answer}\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\n{context_msg}\\n------------\\nGiven the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\nRefined Answer: \"), conditionals=[(<function is_chat_model at 0x130472840>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_msg', 'query_str', 'existing_answer'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.USER: 'user'>, content=\"You are an expert Q&A system that strictly operates in two modes when refining existing answers:\\n1. **Rewrite** an original answer using the new context.\\n2. **Repeat** the original answer if the new context isn't useful.\\nNever reference the original answer or context directly in your answer.\\nWhen in doubt, just repeat the original answer.\\nNew Context: {context_msg}\\nQuery: {query_str}\\nOriginal Answer: {existing_answer}\\nNew Answer: \", additional_kwargs={})]))])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine_gpt35.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = \"datasets/Question-for-second-evaluaton.xlsx\"\n",
    "\n",
    "def rag_query(row, query_engine):\n",
    "    query = row['Question']\n",
    "    response = query_engine.query(query)  # Make sure the query_engine object has a query method\n",
    "    context = [node.text for node in response.source_nodes]\n",
    "    return pd.Series([response.response, context])\n",
    "\n",
    "# query engine baseline gpt35\n",
    "llm_gpt35 = OpenAI(model='gpt-3.5-turbo-0125', api_key=API_KEY, temperature=1)\n",
    "query_engine_gpt35 = index.as_query_engine(similarity_top_k=2, llm=llm_gpt35)\n",
    "\n",
    "# query engine finetuned gpt35\n",
    "llm_emil = OpenAI(model=\"ft:gpt-3.5-turbo-0125:personal:master:9Bfq5w4r\", api_key=API_KEY, temperature=1)\n",
    "query_engine_emil = index.as_query_engine(similarity_top_k=2, llm=llm_emil)\n",
    "\n",
    "# Load your DataFrame\n",
    "df_eval = pd.read_excel(excel_path)\n",
    "\n",
    "for name in ['rag', 'ft_rag']:\n",
    "    if name == 'rag':\n",
    "        query_engine = query_engine_gpt35\n",
    "    if name == 'ft_rag':\n",
    "        query_egnine = query_engine_emil\n",
    "    # Apply the rag_query function across the DataFrame\n",
    "    df_eval[['Answer', 'context']] = df_eval.apply(lambda row: rag_query(row, query_engine), axis=1)\n",
    "    df_eval.to_excel(f'eval_{name}.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
